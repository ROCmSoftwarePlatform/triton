{
  "llama3_8B": {
    "num_attention_heads": 32,
    "num_key_value_heads": 8,
    "hidden_size": 4096,
    "max_ctx_len": 8192,
    "intermediate_size": 14336,
    "vocab_size": 128256
  },
  "llama3_70B": {
    "num_attention_heads": 64,
    "num_key_value_heads": 8,
    "hidden_size": 8192,
    "max_ctx_len": 8192,
    "intermediate_size": 28672,
    "vocab_size": 128256
  },
  "llama3_405B": {
    "num_attention_heads": 128,
    "num_key_value_heads": 8,
    "hidden_size": 16384,
    "max_ctx_len": 8192,
    "intermediate_size": 53248,
    "vocab_size": 128256
  }
}
